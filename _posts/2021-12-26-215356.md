---
layout: post
title: Data preprocessing with Pandas
series: Pandas
---

Data preprocessing is the process of converting or mapping data from one raw format to another one for further analysis (also known as data cleansing or wrangling).

For this chapter, we are going to use previous chapter's dataframe. Let's load it again and define its row header:

```python
import pandas as pd

URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data"
df = pd.read_csv(URL, header=None)

df.columns = [
    "symboling", "normalized-losses", "make", "fuel-type", "aspiration", "num-of-doors", "body-style",
    "drive-wheels", "engine-location", "wheel-base", "length","width", "height", "curb-weight", "engine-type",
    "num-of-cylinders", "engine-size", "fuel-system", "bore", "stroke", "compression-ratio", "horsepower",
    "peak-rpm", "city-mpg", "highway-mpg", "price"
]

print(df.head())
```

```
   symboling normalized-losses         make  ... city-mpg highway-mpg  price
0          3                 ?  alfa-romero  ...       21          27  13495
1          3                 ?  alfa-romero  ...       21          27  16500
2          1                 ?  alfa-romero  ...       19          26  16500
3          2               164         audi  ...       24          30  13950
4          2               164         audi  ...       18          22  17450

[5 rows x 26 columns]
```

Great! Now you can appreciate that there are some elements in the dataframe that contain a `?` character instead of a value. That is a placeholder for a **nonexistent/null value**. The fact that these values exist in the dataframe would not be a problem if they could not modify other values and/or affect the overall behavior of some methods performed over the dataframe... but they actually do. This means that the dataframe must be cleansed prior to the treatment of the data it contains.

***

## 1. Data types

Each dataframe is composed of columns. Each column has a finite number of elements that are classified under a single data type (yet data types between different columns might not be the same). Let's take a look at this dataframe's data types:

```python
print(df.dtypes)
```

```
symboling              int64
normalized-losses     object
make                  object
fuel-type             object
aspiration            object
num-of-doors          object
body-style            object
drive-wheels          object
engine-location       object
wheel-base           float64
length               float64
width                float64
height               float64
curb-weight            int64
engine-type           object
num-of-cylinders      object
engine-size            int64
fuel-system           object
bore                  object
stroke                object
compression-ratio    float64
horsepower            object
peak-rpm              object
city-mpg               int64
highway-mpg            int64
price                 object
dtype: object
```

As you can see, `symboling` is an `int64` type, `width` is a `float64` and some of the other ones are an `object` type. Clearly, both `int64` and `float64` stand for numerical data types. They do not usually cause any trouble when operating with the dataframe. The `object` ones, on the other hand, can be quite conflictive.

In this case, the `object` ones are actually `str` (string) elements. This can either be due to the fact that the data from a column is indeed a string (such as the elements from the `make` column: _alfa-romero_, _audi_, etc.) or due to other reasons, such as in the case of the `normalized-losses` column. You must have noticed that there are some `?` symbols in that column. If you guessed that those symbols must represent a null/undefined value, you are right, that is exactly what they are meant to do.

There is, however, an issue with that. The fact that these symbols are what they are is affecting the data type of the entire column (since operations with numerical elements are allowed, but not with string ones, therefore the operational capabilities of the entire column must be restricted).

***

## 2. Data standarization

Luckily, Pandas provides with a replacement method that covers the whole table, but... what should be the `?` value replaced with? Well, since we are operating with numerical elements (that's what data analysis is based upon), the most obvious way to signify that a value is undefined or non-treatable <!-- Syntax -->is to use the `NaN` symbol, which stands for _Not a Number_. Said value is provided by the `numpy` module, which we will have to import. Having done this, the next step is to replace all the `?` values with `numpy.nan`.

```python
import numpy as np

df.replace('?', np.nan, inplace=True)

print(df.head())
```

```
   symboling normalized-losses         make  ... city-mpg highway-mpg  price
0          3               NaN  alfa-romero  ...       21          27  13495
1          3               NaN  alfa-romero  ...       21          27  16500
2          1               NaN  alfa-romero  ...       19          26  16500
3          2               164         audi  ...       24          30  13950
4          2               164         audi  ...       18          22  17450

[5 rows x 26 columns]
```

You might have noticed the `inplace` parameter located in the `replace` method. This parameter is common to specific methods from the Pandas module: the modifier ones. These are methods that can modify the elements of the dataframe they are called upon. If the `inplace` parameter is not defined or set to `False`, the method call will return the new modified dataframe, while the original one will remain untouched. If the parameter is set to `True`, the modification will take place over the original dataframe, modifying it _in place_.

Remember the `info` method from the last chapter? Let's call it again and see what we get after replacing the `?` symbol with `np.nan`:

```python
df.info()
```

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 205 entries, 0 to 204
Data columns (total 26 columns):
 #   Column             Non-Null Count  Dtype
---  ------             --------------  -----
 0   symboling          205 non-null    int64
 1   normalized-losses  164 non-null    object
 2   make               205 non-null    object
 3   fuel-type          205 non-null    object
 4   aspiration         205 non-null    object
 5   num-of-doors       203 non-null    object
 6   body-style         205 non-null    object
 7   drive-wheels       205 non-null    object
 8   engine-location    205 non-null    object
 9   wheel-base         205 non-null    float64
 10  length             205 non-null    float64
 11  width              205 non-null    float64
 12  height             205 non-null    float64
 13  curb-weight        205 non-null    int64
 14  engine-type        205 non-null    object
 15  num-of-cylinders   205 non-null    object
 16  engine-size        205 non-null    int64
 17  fuel-system        205 non-null    object
 18  bore               201 non-null    object
 19  stroke             201 non-null    object
 20  compression-ratio  205 non-null    float64
 21  horsepower         203 non-null    object
 22  peak-rpm           203 non-null    object
 23  city-mpg           205 non-null    int64
 24  highway-mpg        205 non-null    int64
 25  price              201 non-null    object
dtypes: float64(5), int64(5), object(16)
memory usage: 41.8+ KB
```

If you paid enough attention to the output above, you might have noticed that the non-null counts have changed for some columns, such as `normalized-losses`, `num-of-doors`, `bore` and others. Question is: why did the non-null count evaluate to `205` (meaning there were no null values, since there are `205` rows in total) before, even if there were `?` symbols that meant the value was undefined/null? Well, that is because the `?` symbol does not represent a null value. The `numpy.nan` (also `numpy.NaN`) one does. This is due to the fact that a certain degree of standarization is required in order to provide with a consistent null value detection method. If elements such as `?`, `null`, `none`, etc. were to be counted as null values, there might be many issues in general file processing.

